> ##Problem Statement : An education department in the US needs to analyze the factors that influence
> ##the admission of a student into a college
> ##-------------------------------------------------------------------------------------------------------
> #load all the required libraries
> library(corrplot)
> library(caTools)
> library(ROCR)
> library(rpart)
> library(rpart.plot)
> library(e1071)
> library(caret)
> library(randomForest)
> library(dplyr)
> raw_df = read.csv(choose.files())
> data = raw_df #take the backup of data
> data[1:3,]
  admit gre  gpa ses Gender_Male Race rank
1     0 380 3.61   1           0    3    3
2     1 660 3.67   2           0    2    3
3     1 800 4.00   2           0    2    1
> str(data)
'data.frame':	400 obs. of  7 variables:
 $ admit      : int  0 1 1 1 0 1 1 0 1 0 ...
 $ gre        : int  380 660 800 640 520 760 560 400 540 700 ...
 $ gpa        : num  3.61 3.67 4 3.19 2.93 3 2.98 3.08 3.39 3.92 ...
 $ ses        : int  1 2 2 1 3 2 2 2 1 1 ...
 $ Gender_Male: int  0 0 0 1 1 1 1 0 1 0 ...
 $ Race       : int  3 2 2 2 2 1 2 2 1 2 ...
 $ rank       : int  3 3 1 4 4 2 1 2 3 2 ...
> table(data$admit) ##This is Target variable and is

  0   1 
273 127 
> table(data$gre)

220 300 340 360 380 400 420 440 460 480 500 520 540 560 580 600 620 640 660 680 700 720 740 760 780 800 
  1   3   4   4   8  11   7  10  14  16  21  24  27  24  29  23  30  21  24  20  22  11  11   5   5  25 
> table(data$gpa)

2.26 2.42 2.48 2.52 2.55 2.56 2.62 2.63 2.65 2.67 2.68 2.69  2.7 2.71 2.73 2.76 2.78 2.79 2.81 2.82 2.83 
   1    2    1    1    1    1    2    1    1    2    1    1    2    2    1    1    2    2    3    2    1 
2.84 2.85 2.86 2.87 2.88  2.9 2.91 2.92 2.93 2.94 2.95 2.96 2.97 2.98    3 3.01 3.02 3.03 3.04 3.05 3.06 
   1    2    2    1    2    4    3    2    5    3    1    2    2    6    4    2    4    1    2    3    1 
3.07 3.08 3.09  3.1 3.11 3.12 3.13 3.14 3.15 3.16 3.17 3.18 3.19  3.2 3.21 3.22 3.23 3.24 3.25 3.27 3.28 
   4    4    1    1    1    4    5    4    7    2    5    1    5    2    1    5    3    2    2    3    4 
3.29  3.3 3.31 3.32 3.33 3.34 3.35 3.36 3.37 3.38 3.39  3.4 3.41 3.42 3.43 3.44 3.45 3.46 3.47 3.48 3.49 
   2    4    8    4    5    5    7    4    3    5    3    7    1    1    5    3    7    5    3    3    5 
 3.5 3.51 3.52 3.53 3.54 3.55 3.56 3.57 3.58 3.59  3.6 3.61 3.62 3.63 3.64 3.65 3.66 3.67 3.69  3.7 3.71 
   4    5    4    2    3    1    3    3    5    5    3    3    2    6    5    4    1    4    3    3    2 
3.72 3.73 3.74 3.75 3.76 3.77 3.78  3.8 3.81 3.82 3.83 3.84 3.85 3.86 3.87 3.88 3.89  3.9 3.91 3.92 3.93 
   1    2    4    2    2    5    4    2    3    1    1    2    1    2    1    3    3    3    1    2    1 
3.94 3.95 3.97 3.98 3.99    4 
   5    5    1    1    3   28 
> boxplot(data$gpa)
> table(data$ses)

  1   2   3 
132 139 129 
> table(data$Gender_Male)

  0   1 
210 190 
> table(data$Race)

  1   2   3 
143 129 128 
> table(data$rank)

  1   2   3   4 
 61 151 121  67 
> boxplot(data$gre)
> summary(data$gre)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  220.0   520.0   580.0   587.7   660.0   800.0 
> #Check if there are any null records
> colSums(is.na(data))
      admit         gre         gpa         ses Gender_Male        Race        rank 
          0           0           0           0           0           0           0 
> cor(data)
                  admit          gre          gpa          ses  Gender_Male        Race         rank
admit        1.00000000  0.184434277  0.178212254 -0.046857635 -0.025004220 -0.06033479 -0.242513176
gre          0.18443428  1.000000000  0.384265878 -0.033181992  0.006812595 -0.04702756 -0.123447073
gpa          0.17821225  0.384265878  1.000000000  0.006837625 -0.017135766  0.05830253 -0.057460768
ses         -0.04685763 -0.033181992  0.006837625  1.000000000 -0.028355226 -0.05312133  0.008055266
Gender_Male -0.02500422  0.006812595 -0.017135766 -0.028355226  1.000000000 -0.05403541 -0.027332583
Race        -0.06033479 -0.047027565  0.058302528 -0.053121333 -0.054035410  1.00000000  0.036342349
rank        -0.24251318 -0.123447073 -0.057460768  0.008055266 -0.027332583  0.03634235  1.000000000
> corrplot(cor(data))
> #Split the data into train and test
> set.seed(100)
> split = sample.split(Y = data$admit, SplitRatio = .7)
> training = raw_df[split,]
> test = raw_df[!split,]
> remove(split)
> #Use logistic regression using family="binomial" as admit variables has only 2 categories
> lm_logreg = glm(formula = as.factor(admit) ~ ., family = "binomial", data = training)
> summary(lm_logreg)

Call:
glm(formula = as.factor(admit) ~ ., family = "binomial", data = training)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.4763  -0.8770  -0.5973   1.1286   2.1793  

Coefficients:
             Estimate Std. Error z value Pr(>|z|)    
(Intercept) -3.688929   1.496873  -2.464   0.0137 *  
gre          0.002434   0.001331   1.829   0.0674 .  
gpa          0.996551   0.412326   2.417   0.0157 *  
ses         -0.083341   0.171379  -0.486   0.6268    
Gender_Male -0.283450   0.275765  -1.028   0.3040    
Race         0.027778   0.166409   0.167   0.8674    
rank        -0.695166   0.162607  -4.275 1.91e-05 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 350.14  on 279  degrees of freedom
Residual deviance: 315.41  on 273  degrees of freedom
AIC: 329.41

Number of Fisher Scoring iterations: 4

> lm_logreg_step1 = step(object = lm_logreg, direction = "both")
Start:  AIC=329.41
as.factor(admit) ~ gre + gpa + ses + Gender_Male + Race + rank

              Df Deviance    AIC
- Race         1   315.44 327.44
- ses          1   315.65 327.65
- Gender_Male  1   316.47 328.47
<none>             315.41 329.41
- gre          1   318.83 330.83
- gpa          1   321.46 333.46
- rank         1   335.91 347.91

Step:  AIC=327.44
as.factor(admit) ~ gre + gpa + ses + Gender_Male + rank

              Df Deviance    AIC
- ses          1   315.69 325.69
- Gender_Male  1   316.53 326.53
<none>             315.44 327.44
- gre          1   318.83 328.83
+ Race         1   315.41 329.41
- gpa          1   321.50 331.50
- rank         1   335.92 345.92

Step:  AIC=325.69
as.factor(admit) ~ gre + gpa + Gender_Male + rank

              Df Deviance    AIC
- Gender_Male  1   316.76 324.76
<none>             315.69 325.69
- gre          1   319.10 327.10
+ ses          1   315.44 327.44
+ Race         1   315.65 327.65
- gpa          1   321.62 329.62
- rank         1   335.92 343.92

Step:  AIC=324.76
as.factor(admit) ~ gre + gpa + rank

              Df Deviance    AIC
<none>             316.76 324.76
+ Gender_Male  1   315.69 325.69
- gre          1   320.10 326.10
+ ses          1   316.53 326.53
+ Race         1   316.69 326.69
- gpa          1   323.03 329.03
- rank         1   336.67 342.67
> summary(lm_logreg_step1)

Call:
glm(formula = as.factor(admit) ~ gre + gpa + rank, family = "binomial", 
    data = training)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.5697  -0.8856  -0.6120   1.1140   2.2669  

Coefficients:
             Estimate Std. Error z value Pr(>|z|)    
(Intercept) -3.987027   1.402625  -2.843  0.00448 ** 
gre          0.002398   0.001327   1.806  0.07085 .  
gpa          1.005352   0.409067   2.458  0.01398 *  
rank        -0.677324   0.160326  -4.225 2.39e-05 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 350.14  on 279  degrees of freedom
Residual deviance: 316.76  on 276  degrees of freedom
AIC: 324.76

Number of Fisher Scoring iterations: 4

> remove(lm_logreg)
> #Decide the threshold using ROC curve
> ROCRpred = prediction(predictions = lm_logreg_step1$fitted.values,labels = training$admit)
> ROCRperf = performance(prediction.obj = ROCRpred, "tpr", "fpr")
> plot(ROCRperf, colorize = T, print.cutoffs.at = seq(0,1,.025), text.adj=c(-.2,1.7))
> rm(ROCRpred, ROCRperf)
> #predict the probability values for the test data
> predict_test = predict(object = lm_logreg_step1, newdata = test, type = "response")
> final_predict = ifelse(predict_test >= 0.38, 1, 0)
> table(final_predict)
final_predict
 0  1 
74 46 
> #confusion matrix to check the accuracy model
> table(actual = test$admit, predicted = final_predict)->tab
> accuracy = (tab[1,1]+tab[2,2])/nrow(test)
> accuracy
[1] 0.65
> result = data.frame("Model" = "Logistic Regression", "Accuracy" = round(accuracy * 100, digits = 2))
> result
                Model Accuracy
1 Logistic Regression       65
> rm(lm_logreg_step1, accuracy, final_predict, predict_test, tab)
> #Decision Trees----------------------------------------------------------------
> lm_dt = rpart(as.factor(admit)~.,data = training )
> prp(lm_dt)
> plotcp(lm_dt)
> mod = prune(tree = lm_dt, cp = .021)
> prp(mod)
> #make predictions on test data
> predict_test = predict(object = mod, newdata = test, type = "class")
> #confusion matrix
> table(predicted = predict_test, actual = test$admit)->tab
> accuracy = (tab[1,1]+tab[2,2])/nrow(test)
> round(accuracy * 100, digits = 2)
[1] 69.17
> result = rbind(result, data.frame("Model" = "Decision Tree", "Accuracy" = round(accuracy * 100, digits = 2)))
> result
                Model Accuracy
1 Logistic Regression    65.00
2       Decision Tree    69.17
> rm(lm_dt, mod, accuracy, predict_test, tab)
> #We want to standardize our columns
> head(training)
  admit gre  gpa ses Gender_Male Race rank
1     0 380 3.61   1           0    3    3
2     1 660 3.67   2           0    2    3
5     0 520 2.93   3           1    2    4
6     1 760 3.00   2           1    1    2
7     1 560 2.98   2           1    2    1
8     0 400 3.08   2           0    2    2
> training_scaled = scale(training[,c(2, 3)])
> training_scaled = cbind(training[,c(1,4,5,6,7)], training_scaled)
> head(test)
   admit gre  gpa ses Gender_Male Race rank
3      1 800 4.00   2           0    2    1
4      1 640 3.19   1           1    2    4
9      1 540 3.39   1           1    1    3
14     0 700 3.08   2           0    2    2
21     0 500 3.17   3           0    2    3
27     1 620 3.61   2           0    1    1
> test_scaled = scale(test[,c(2, 3)])
> test_scaled = cbind(test[,c(1,4,5,6,7)], test_scaled)
> head(training_scaled)
  admit ses Gender_Male Race rank        gre        gpa
1     0   1           0    3    3 -1.8257499  0.6106701
2     1   2           0    2    3  0.6404595  0.7734122
5     0   3           1    2    4 -0.5926452 -1.2337396
6     1   2           1    1    2  1.5212485 -1.0438739
7     1   2           1    2    1 -0.2403296 -1.0981213
8     0   2           0    2    2 -1.6495920 -0.8268845
> svm_cl = svm(as.factor(admit)~., training_scaled, type = 'C-classification', kernel = 'linear')
> prediction = predict(svm_cl,test_scaled)
> table(prediction, test$admit)->tab
> tab
          
prediction  0  1
         0 82 38
         1  0  0
> (tab[1,1]+tab[2,2])/nrow(test)->accuracy
> accuracy
[1] 0.6833333
> result = rbind(result, data.frame("Model" = "SVM Linear", "Accuracy" = round(accuracy * 100, digits = 2)))
> result
                Model Accuracy
1 Logistic Regression    65.00
2       Decision Tree    69.17
3          SVM Linear    68.33
> rm(svm_cl, accuracy, prediction, tab, training)
> #support vector machines - radial
> svm_radial_cf = svm(as.factor(admit)~., training_scaled, type = 'C-classification', kernel = 'radial')
> prediction = predict(svm_radial_cf,test_scaled)
> table(prediction, test$admit)->tab
> tab
          
prediction  0  1
         0 72 30
         1 10  8
> (tab[1,1]+tab[2,2])/nrow(test)->accuracy
> round(accuracy * 100, digits = 2)
[1] 66.67
> result = rbind(result, data.frame("Model" = "SVM radial", "Accuracy" = round(accuracy * 100, digits = 2)))
> result
                Model Accuracy
1 Logistic Regression    65.00
2       Decision Tree    69.17
3          SVM Linear    68.33
4          SVM radial    66.67
> rm(svm_radial_cf, test, test_scaled, training_scaled, accuracy, prediction, tab)
> #k-fold cross validation - to check the performance of the model on variance error
> k = 10
> folds = createFolds(y = data$admit,k = k)
> fn = function(x)#x will take each and every fold one-by-one
+ {
+   training_fold = data[-x,]# will have all the rows in raw_df except for row numbers in x
+   test_fold = data[x,]#we will have all rows from raw_df db having row numbers = x
+   
+   rmf_cl = randomForest(as.factor(admit)~.,data = training_fold, type = 'C-classification')
+   predict(rmf_cl, test_fold) -> prediction
+   
+   table(actual = test_fold$admit, predicted = prediction)-> tab
+   accuracy = (tab[1,1]+tab[2,2])/nrow(test_fold)
+   return(accuracy)
+ }
> lapply(X = folds, fn)->cv
> unlist(cv)
Fold01 Fold02 Fold03 Fold04 Fold05 Fold06 Fold07 Fold08 Fold09 Fold10 
 0.575  0.725  0.725  0.725  0.750  0.700  0.600  0.775  0.625  0.750 
> mean(unlist(cv))
[1] 0.695
> round(mean(unlist(cv)) * 100, digits = 2)
[1] 69.5
> ##This model is 70.75% Accurate
> result = rbind(result, data.frame("Model" = "Random Forest", "Accuracy" = round(mean(unlist(cv)) * 100, digits = 2)))
> result
                Model Accuracy
1 Logistic Regression    65.00
2       Decision Tree    69.17
3          SVM Linear    68.33
4          SVM radial    66.67
5       Random Forest    69.50
> remove(cv, folds, k, fn)
> #Recategorise the gre as Low, Medium, High
> data[1:6,]
  admit gre  gpa ses Gender_Male Race rank
1     0 380 3.61   1           0    3    3
2     1 660 3.67   2           0    2    3
3     1 800 4.00   2           0    2    1
4     1 640 3.19   1           1    2    4
5     0 520 2.93   3           1    2    4
6     1 760 3.00   2           1    1    2
> data$gre_category = cut(x = data$gre, breaks = c(0, 439, 579, max(data$gre)), labels = c("Low", "Medium", "High") )
> data[1:6,]
  admit gre  gpa ses Gender_Male Race rank gre_category
1     0 380 3.61   1           0    3    3          Low
2     1 660 3.67   2           0    2    3         High
3     1 800 4.00   2           0    2    1         High
4     1 640 3.19   1           1    2    4         High
5     0 520 2.93   3           1    2    4       Medium
6     1 760 3.00   2           1    1    2         High
> PT_final = summarise(.data = group_by(data, gre_category), sum = sum(admit), length = length(admit), prob = sum(admit)/length(admit))
> PT_final
# A tibble: 3 x 4
  gre_category   sum length  prob
  <fct>        <int>  <int> <dbl>
1 Low              4     38 0.105
2 Medium          39    136 0.287
3 High            84    226 0.372
> ggplot(PT_final,aes(x=gre_category,y=prob))+geom_point()
> table(data$admit, data$gre_category)
   
    Low Medium High
  0  34     97  142
  1   4     39   84